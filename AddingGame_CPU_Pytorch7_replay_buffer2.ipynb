{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emm32449/MCTS-in-Python/blob/main/AddingGame_CPU_Pytorch7_replay_buffer2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAZmLtNwwWoM"
      },
      "source": [
        "The game is a simple adding game. The game starts at 0 and players can make moves by adding 1, 2, or 3 to the current state. The goal is to reach exactly 10. If a player’s move results in a state exceeding 10, they receive a negative reward. If the state is less than 10, the reward is based on how close the state is to 10. The game resets once it reaches or exceeds 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x0QYmzhw8vvl"
      },
      "outputs": [],
      "source": [
        "# Assume we have a simple game state\n",
        "class GameState:\n",
        "    def __init__(self, state):\n",
        "        self.state = state\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.state)\n",
        "\n",
        "    def get_legal_moves(self):\n",
        "        return [1, 2, 3]\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return GameState(0)\n",
        "\n",
        "    def make_move(self, move):\n",
        "        return GameState(self.state + move)\n",
        "\n",
        "    def is_terminal(self):\n",
        "        return self.state >= 10\n",
        "\n",
        "    def get_reward(self):\n",
        "        # Give a positive reward if the state is exactly 10\n",
        "        if self.state == 10:\n",
        "            return 1\n",
        "        # Give a negative reward if the state exceeds 10\n",
        "        elif self.state > 10:\n",
        "            return -1\n",
        "        # Give a reward based on how close the state is to 10\n",
        "        else:\n",
        "            return 1 - abs(self.state - 10) / 10\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "\n",
        "    def copy(self):\n",
        "        return GameState(self.state)\n",
        "\n",
        "    def to_array(self):\n",
        "        return [self.state]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFNyawMAyaHJ"
      },
      "source": [
        "Alpha Go Zero uses a variant of Monte Carlo Tree Search (MCTS) within its training loop. Starting from the root node, which represents the current game state, it traverses the tree until it reaches a leaf node. This game state is passed through the neural network, which outputs a vector of move probabilities and a scalar estimating the expected outcome. The leaf node is then expanded, with each legal move becoming a new child node. The move probabilities output by the network initialize the prior probabilities of selecting each child node in future simulations. The estimated value output by the network is backpropagated up the tree, updating the value estimates of all nodes along the traversed path.\n",
        "\n",
        "This process is repeated for many simulations, with the Upper Confidence Bound (UCB) formula guiding the selection of nodes during tree traversal. The UCB formula balances exploration and exploitation, taking into account both the value of the node and the prior probability of the node, as well as the number of times the node has been visited. After a large number of simulations, the move leading to the child node with the highest visit count from the root is selected as the next action. This procedure allows AlphaGo Zero to effectively learn a policy favoring high-value actions and a value function predicting the game outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sOOxRknh73dp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Normalize GameState Data\n",
        "max_state_value = 15  # Initialize with a known max value or a reasonable estimate\n",
        "\n",
        "def prepare_data(state):\n",
        "    global max_state_value\n",
        "    max_state_value = max(max_state_value, max(state))  # Update the max value if the current state is larger\n",
        "    normalized_state = [s / max_state_value for s in state]\n",
        "    return normalized_state\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, game_state, policy, parent=None, move=None):\n",
        "        self.game_state = game_state\n",
        "        self.policy = policy\n",
        "        self.parent = parent\n",
        "        self.move = move\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.wins = 0\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"GameState: {self.game_state}, Move: {self.move}, Visits: {self.visits}, Wins: {self.wins}\"\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, root):\n",
        "        self.model = model\n",
        "        self.root = root\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.1)\n",
        "\n",
        "    def choose_action(self, node):\n",
        "        # Check the mode of the model\n",
        "        if self.model.training:\n",
        "            # If the model is in training mode, use probabilistic action selection\n",
        "            visit_counts = np.array([child.visits for child in node.children])\n",
        "            visit_dist = visit_counts / visit_counts.sum()\n",
        "            chosen_action = node.children[np.random.choice(len(node.children), p=visit_dist)].move\n",
        "        else:\n",
        "            # If the model is in evaluation mode, use deterministic action selection\n",
        "            max_visits = max(child.visits for child in node.children)\n",
        "            chosen_action = [child.move for child in node.children if child.visits == max_visits][0]\n",
        "        return chosen_action\n",
        "\n",
        "    def selection(self, node):\n",
        "        while len(node.children) > 0:\n",
        "            max_value = -math.inf\n",
        "            selected_node = None\n",
        "            for child in node.children:\n",
        "                if child.visits == 0:  # Stop if the child node has not been visited yet\n",
        "                    return child\n",
        "                state_tensor = torch.tensor(child.game_state.to_array(), dtype=torch.float32)\n",
        "                policy, value = self.model(state_tensor)\n",
        "                Q = child.wins / child.visits if child.visits != 0 else 0\n",
        "                U = policy.mean() * math.sqrt(node.visits) / (1 + child.visits)\n",
        "                node_value = Q + U + child.policy.mean()  # Take the mean of the policy tensor\n",
        "                if node_value > max_value:\n",
        "                    max_value = node_value\n",
        "                    selected_node = child\n",
        "            node = selected_node\n",
        "        return node\n",
        "\n",
        "    def expansion(self, node):\n",
        "        # Get the list of legal moves from the game state\n",
        "        legal_moves = node.game_state.get_legal_moves()\n",
        "\n",
        "        # For each legal move, create a new node and add it to the children of the current node\n",
        "        for move in legal_moves:\n",
        "            new_game_state = node.game_state.make_move(move)\n",
        "            state_tensor = torch.tensor(new_game_state.to_array(), dtype=torch.float32)\n",
        "            policy, value = self.model(state_tensor)\n",
        "            child_node = Node(new_game_state, policy, parent=node, move=move)\n",
        "            node.children.append(child_node)\n",
        "\n",
        "        return node.children\n",
        "\n",
        "    def simulation(self, node):\n",
        "        game_state = node.game_state.copy()\n",
        "        while not game_state.is_terminal():\n",
        "            state_tensor = torch.tensor(game_state.to_array(), dtype=torch.float32)\n",
        "            policy, _ = self.model(state_tensor)\n",
        "            policy_dist = F.softmax(policy, dim=0).detach().numpy()\n",
        "            legal_moves = game_state.get_legal_moves()\n",
        "            move = np.random.choice(legal_moves, p=policy_dist)\n",
        "            game_state = game_state.make_move(move)\n",
        "            print(f\"Game state after move: {game_state}\")  # Print the game state after each move\n",
        "        reward = game_state.get_reward()\n",
        "        print(f\"Terminal game state reached with reward: {reward}\")  # Print the reward when a terminal state is reached\n",
        "        return reward\n",
        "\n",
        "    def backpropagation(self, node, reward):\n",
        "        # While node is not None\n",
        "        while node is not None:\n",
        "            # Update the visit count of the node\n",
        "            node.visits += 1\n",
        "\n",
        "            # Update the win count of the node\n",
        "            node.wins += reward\n",
        "\n",
        "            # Move to the parent node\n",
        "            node = node.parent\n",
        "\n",
        "    def print_tree(self, node, indent=\"\"):\n",
        "        print(indent + str(node))\n",
        "        for child in node.children:\n",
        "            self.print_tree(child, indent + \"  \")\n",
        "\n",
        "    # Train\n",
        "    def train(self, states, policies, rewards, epochs):\n",
        "        # Convert states, policies, and rewards to tensors\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        policies = torch.tensor(policies, dtype=torch.float32)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "\n",
        "        for _ in range(epochs):\n",
        "            policy_pred, value_pred = self.model(states)\n",
        "\n",
        "            policy_loss = F.kl_div(F.log_softmax(policy_pred, dim=1), policies)\n",
        "            value_loss = F.mse_loss(value_pred.squeeze(), rewards)\n",
        "            loss = policy_loss + value_loss\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    # Self Play\n",
        "    def self_play(self, network, game, game_number, num_simulations):\n",
        "        states = []\n",
        "        policies = []\n",
        "        current_state = game.get_initial_state()\n",
        "        root = Node(current_state, None)\n",
        "\n",
        "        for sim in range(num_simulations):\n",
        "            print(f\"Starting simulation.\")\n",
        "            leaf = self.selection(root)\n",
        "            children = self.expansion(leaf)\n",
        "            self.simulation(random.choice(children))\n",
        "            legal_actions = game.get_legal_moves()\n",
        "            visit_counts = [child.visits if child.visits > 0 else torch.rand(1).item() for child in root.children]\n",
        "            policy = torch.zeros(network.policy_head.out_features)  # Initialize a zero vector of length equal to the output size of the policy head\n",
        "            for action, visit_count in zip(legal_actions, visit_counts):\n",
        "                policy[action - 1] = visit_count  # Subtract 1 from action to use it as an index\n",
        "            policy = F.softmax(policy, dim=0)\n",
        "\n",
        "            action_index = torch.multinomial(policy, 1).item()\n",
        "            action = root.children[action_index].move\n",
        "            states.append(prepare_data(current_state.to_array()))\n",
        "            policies.append(policy.tolist())  # Convert policy tensor to list\n",
        "            current_state = current_state.make_move(action + 1)  # Add 1 to action to use it as a move\n",
        "            root = root.children[action_index]\n",
        "\n",
        "        # Determine the reward based on the final state of the game\n",
        "        reward = current_state.get_reward()\n",
        "\n",
        "        return states, policies, reward  # Return policies as a list\n",
        "\n",
        "class MyReplayBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        # Initialize your buffer with a maximum size\n",
        "        self.max_size = max_size\n",
        "        self.buffer = []\n",
        "\n",
        "    def add_transition(self, transition):\n",
        "        # Add a transition (state, action, reward, next_state) to the buffer\n",
        "        self.buffer.append(transition)\n",
        "        if len(self.buffer) > self.max_size:\n",
        "            self.buffer.pop(0)\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        # Sample a batch of transitions\n",
        "        return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DtoYH_eB9lr"
      },
      "source": [
        "Initial Training Loop:\n",
        "\n",
        "A Replay Buffer helps to break the correlation between consecutive experiences, which is beneficial for the stability of the learning algorithm. The policy is represented implicitly by the actions chosen by the Monte Carlo Tree Search, which uses the neural network’s outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc9tvyZeB6zh"
      },
      "outputs": [],
      "source": [
        "# Create the game and network\n",
        "game = GameState(0)  # Initialize the game\n",
        "num_actions = len(game.get_legal_moves())  # Get the number of possible actions\n",
        "network = PolicyValueResNet(num_actions)  # Initialize the policy-value network\n",
        "uniform_policy = [1.0 / num_actions] * num_actions  # Initialize a uniform policy\n",
        "root = Node(game.get_initial_state(), uniform_policy)  # Initialize the root node of the MCTS\n",
        "mcts = MCTS(network, root)  # Initialize the MCTS with the network and root node\n",
        "epoch_losses = []  # Initialize a list to store the losses per epoch\n",
        "network.train()  # Set the network to training mode\n",
        "\n",
        "game_number = 1  # Initialize the game number\n",
        "optimizer = torch.optim.Adam(network.parameters())  # Initialize the optimizer\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)  # Initialize the learning rate scheduler\n",
        "\n",
        "import random\n",
        "\n",
        "total_games = 0  # Initialize the total number of games played\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 5\n",
        "num_simulations = 5\n",
        "# Replay Buffer\n",
        "max_buffer_size = 1000\n",
        "replay_buffer = MyReplayBuffer(max_buffer_size)\n",
        "batch_size = 10\n",
        "subset_size = 8\n",
        "\n",
        "for i in range(epochs):\n",
        "    batch_states, batch_policies, batch_rewards = [], [], []\n",
        "    batch_losses = []\n",
        "\n",
        "    for j in range(batch_size):\n",
        "        # Collect transitions during self-play\n",
        "        states, policies, reward = mcts.self_play(network, game, game_number, num_simulations)\n",
        "        for state, policy, r in zip(states, policies, [reward] * len(states)):\n",
        "            replay_buffer.add_transition((state, policy, r))\n",
        "\n",
        "        total_games += 1\n",
        "        print(f\"Game number {total_games} completed in batch {i + 1}\")\n",
        "\n",
        "    # Sample a subset from the buffer\n",
        "    batch_transitions = replay_buffer.sample_batch(subset_size)\n",
        "    subset_states, subset_policies, subset_rewards = zip(*batch_transitions)\n",
        "\n",
        "    # Train the network using the subset\n",
        "    loss = mcts.train(subset_states, subset_policies, subset_rewards, epochs)\n",
        "    batch_losses.append(loss)\n",
        "\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    epoch_losses.append(avg_loss)\n",
        "    print(f\"Average loss after epoch {i + 1}: {avg_loss}\")\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRFKPBfHv8ZN"
      },
      "source": [
        "Network:\n",
        "⌨\n",
        "\n",
        "Input is a 1D vector rather than a 2D Board or image. Adjust the network to use fully connected layers (nn.Linear) instead of 2D convolutional layers (nn.Conv2d). The idea behind a Residual Block is to introduce a so-called “skip connection” or “shortcut”, which allows the gradient to be directly backpropagated to earlier layers.\n",
        "\n",
        "During training, the network is updated to make its policy output match the actions selected by the Monte Carlo Tree Search (MCTS), and to make its value output match the final outcome of the simulated games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pru2HloYByJW"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(num_channels, num_channels)\n",
        "        self.fc2 = nn.Linear(num_channels, num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(out)\n",
        "        out += x  # Skip connection\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class PolicyValueResNet(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "        self.hidden1 = nn.Linear(1, 64)\n",
        "        self.resblock = ResidualBlock(64)\n",
        "        self.hidden2 = nn.Linear(64, 64)\n",
        "\n",
        "        # Policy Head forms probabilities for each action, distribution\n",
        "        self.policy_head = nn.Linear(64, num_actions)\n",
        "\n",
        "        # Value Head predicts winner of game from each position, scaler\n",
        "        self.value_head = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.hidden1(state))\n",
        "        x = self.resblock(x)\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        policy = F.softmax(self.policy_head(x), dim=-1)\n",
        "        value = torch.tanh(self.value_head(x))\n",
        "        return policy, value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAF2Ys-ZCFdZ"
      },
      "source": [
        "Save - AddingGame_MCTS_Model2.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5h2irDtCI1h",
        "outputId": "c40acdc8-768b-40eb-8e7d-14e494bc4931"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save the model, optimizer, game number, average losses per epoch, total games, game state, and root node\n",
        "torch.save({\n",
        "    'model_state_dict': network.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'game_number': game_number,\n",
        "    'epoch_losses': epoch_losses,\n",
        "    'total_games': total_games,\n",
        "    'game_state': game,\n",
        "    'root_node': root\n",
        "}, '/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXPZBNPXCU7U"
      },
      "source": [
        "Load & Continue - AddingGame_MCTS_Model2.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nwmUGLMZCZqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6980473-97c8-4e8b-df07-fea5ef0e54c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 151 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 152 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 2\n",
            "Game state after move: 3\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 153 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 154 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 155 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 156 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 157 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 158 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 159 completed in batch 1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 160 completed in batch 1\n",
            "Average loss after epoch 1: 0.008261128328740597\n",
            "Starting simulation.\n",
            "Game state after move: 2\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 161 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 162 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 163 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 164 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 165 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 166 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 167 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 168 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 169 completed in batch 2\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 170 completed in batch 2\n",
            "Average loss after epoch 2: 0.006285958923399448\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 171 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 172 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 173 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 2\n",
            "Game state after move: 3\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 174 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 175 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 176 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 177 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 178 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 2\n",
            "Game state after move: 3\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 179 completed in batch 3\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 180 completed in batch 3\n",
            "Average loss after epoch 3: 0.008003352209925652\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 181 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 182 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 183 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 184 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 185 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 2\n",
            "Game state after move: 3\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 186 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 187 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 2\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 188 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 189 completed in batch 4\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 190 completed in batch 4\n",
            "Average loss after epoch 4: 0.004939745645970106\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 191 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 192 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 193 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Game number 194 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 195 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 7\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 196 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 9\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 197 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 5\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 198 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 5\n",
            "Game state after move: 6\n",
            "Game state after move: 7\n",
            "Game state after move: 8\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 8\n",
            "Game state after move: 9\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 199 completed in batch 5\n",
            "Starting simulation.\n",
            "Game state after move: 3\n",
            "Game state after move: 4\n",
            "Game state after move: 7\n",
            "Game state after move: 10\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Game state after move: 6\n",
            "Game state after move: 8\n",
            "Game state after move: 11\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Game state after move: 12\n",
            "Terminal game state reached with reward: -1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: 1\n",
            "Starting simulation.\n",
            "Terminal game state reached with reward: -1\n",
            "Game number 200 completed in batch 5\n",
            "Average loss after epoch 5: 0.016430597752332687\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "game = GameState(0)  # Initialize the game\n",
        "num_actions = len(game.get_legal_moves())  # Get the number of possible actions\n",
        "network = PolicyValueResNet(num_actions)  # Initialize the policy-value network\n",
        "uniform_policy = [1.0 / num_actions] * num_actions  # Initialize a uniform policy\n",
        "root = Node(game.get_initial_state(), uniform_policy)  # Initialize the root node of the MCTS\n",
        "mcts = MCTS(network, root)  # Initialize the MCTS with the network and root node\n",
        "\n",
        "optimizer = torch.optim.Adam(network.parameters())  # Initialize the optimizer\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)  # Initialize the learning rate scheduler\n",
        "\n",
        "# Load the saved model, optimizer, scheduler, and other necessary variables from the checkpoint\n",
        "checkpoint = torch.load('/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.pth')\n",
        "network.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "game_number = checkpoint['game_number']\n",
        "epoch_losses = checkpoint['epoch_losses']\n",
        "total_games = checkpoint['total_games']\n",
        "game = checkpoint['game_state']\n",
        "root = checkpoint['root_node']\n",
        "\n",
        "# Continue training\n",
        "network.train()  # Ensure the network is in training mode\n",
        "epochs = 5\n",
        "num_simulations = 5\n",
        "max_buffer_size = 1000\n",
        "replay_buffer = MyReplayBuffer(max_buffer_size)  # Initialize the replay buffer\n",
        "batch_size = 10\n",
        "subset_size = 8\n",
        "for i in range(epochs):\n",
        "    batch_states, batch_policies, batch_rewards = [], [], []\n",
        "    batch_losses = []\n",
        "    for j in range(batch_size):\n",
        "        # Perform self-play and add the resulting states, policies, and rewards to the batch\n",
        "        states, policies, reward = mcts.self_play(network, game, game_number, num_simulations)\n",
        "        for state, policy, r in zip(states, policies, [reward] * len(states)):\n",
        "            replay_buffer.add_transition((state, policy, r))  # Add transitions to the replay buffer\n",
        "        total_games += 1\n",
        "        print(f\"Game number {total_games} completed in batch {i + 1}\")\n",
        "    # Sample a random subset of transitions from the replay buffer\n",
        "    batch_transitions = replay_buffer.sample_batch(subset_size)\n",
        "    subset_states, subset_policies, subset_rewards = zip(*batch_transitions)\n",
        "    # Train the network on the subset and calculate the loss\n",
        "    loss = mcts.train(subset_states, subset_policies, subset_rewards, epochs)\n",
        "    batch_losses.append(loss)\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    epoch_losses.append(avg_loss)\n",
        "    print(f\"Average loss after epoch {i + 1}: {avg_loss}\")\n",
        "    scheduler.step()  # Update the learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgQNH257CsqR"
      },
      "source": [
        "Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UPoohsmUCrX4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "9d70c120-76e9-4382-b46b-34ece6374c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Loss: 0.04765557351056486, Max Loss: 0.48564133048057556, Min Loss: 0.004939745645970106\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print and plot the average losses per epoch\n",
        "print(f\"Average Loss: {sum(epoch_losses) / len(epoch_losses)}, Max Loss: {max(epoch_losses)}, Min Loss: {min(epoch_losses)}\")\n",
        "plt.plot(range(1, len(epoch_losses) + 1), epoch_losses)  # Use epoch numbers as x-axis\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "plt.title('Average Loss per Epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjoxJ-3iCvEf"
      },
      "source": [
        "Play, Eval() - AddingGame_MCTS_Model2.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwqfjyLTCyTP"
      },
      "outputs": [],
      "source": [
        "# Initialize a list to store the results of each game\n",
        "win_rates = []\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# See the model playout 10 games using MCTS\n",
        "\n",
        "# Load the model and optimizer\n",
        "checkpoint = torch.load('/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.pth')\n",
        "network.load_state_dict(checkpoint['model_state_dict'])\n",
        "mcts.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "network.eval()\n",
        "\n",
        "# Simulate 10 games\n",
        "num_games = 10\n",
        "\n",
        "for i in range(num_games):\n",
        "    print(f\"Game {i + 1}:\")\n",
        "\n",
        "    # Initialize the game and the root node\n",
        "    game_state = game.get_initial_state()\n",
        "    root = Node(game_state, None)\n",
        "\n",
        "    #Make sure that the MCTS and the root node persist throughout each game\n",
        "    mcts = MCTS(network, root)\n",
        "\n",
        "    while not game_state.is_terminal():\n",
        "        # Perform MCTS simulations from the root\n",
        "        for _ in range(num_games):\n",
        "            leaf = mcts.selection(root)\n",
        "            children = mcts.expansion(leaf)\n",
        "            reward = mcts.simulation(random.choice(children))\n",
        "            mcts.backpropagation(leaf, reward)\n",
        "\n",
        "        # Choose the action that leads to the most visited child node\n",
        "        action = mcts.choose_action(root)\n",
        "\n",
        "        # Apply the action to get the next state\n",
        "        game_state = game_state.make_move(action)\n",
        "\n",
        "        print(f\"Action taken: {action}\")\n",
        "\n",
        "    print(f\"Final reward: {reward}\")\n",
        "\n",
        "    # Append the result of the game to the win_rates list\n",
        "    # Assume a reward of 1 is a win, 0 is a draw, and -1 is a loss\n",
        "    win_rates.append(reward)\n",
        "\n",
        "# mcts.print_tree(mcts.root)\n",
        "\n",
        "# Calculate the win rate\n",
        "win_rate = win_rates.count(1) / num_games\n",
        "\n",
        "# Print and plot the win rates\n",
        "print(f\"Win Rate: {win_rate}\")\n",
        "plt.plot(range(1, num_games + 1), win_rates)\n",
        "plt.xlabel('Game')\n",
        "plt.ylabel('Win Rate')\n",
        "plt.title('Win Rate per Game')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgeOb1b0XCQNDiRWDri5ea",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}