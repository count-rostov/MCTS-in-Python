{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtiVMbBBFLjEnAMIC2wtR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emm32449/MCTS-in-Python/blob/main/AddingGame_T4_GPU_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGagpvrGcGXh"
      },
      "outputs": [],
      "source": [
        "# Assume we have a simple game state\n",
        "class GameState:\n",
        "    def __init__(self, state):\n",
        "        self.state = state\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.state)\n",
        "\n",
        "    def get_legal_moves(self):\n",
        "        return [0, 1, 2, 3]\n",
        "        #return [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3]\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return GameState(0)\n",
        "\n",
        "    def make_move(self, move):\n",
        "        return GameState(self.state + move)\n",
        "\n",
        "    def is_terminal(self):\n",
        "        return self.state >= 10\n",
        "\n",
        "    def get_reward(self):\n",
        "        # Give a positive reward if the state is exactly 10\n",
        "        if self.state == 10:\n",
        "            return 1\n",
        "        # Give a negative reward if the state exceeds 10\n",
        "        elif self.state > 10:\n",
        "            return -1\n",
        "        # Give a reward based on how close the state is to 10\n",
        "        else:\n",
        "            return 1 - abs(self.state - 10) / 10\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "\n",
        "    def copy(self):\n",
        "        return GameState(self.state)\n",
        "\n",
        "    def to_array(self):\n",
        "        return [self.state]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you plan to process multiple samples at once (in a batch), you’ll need to ensure that your tensors are correctly shaped. For example, if you want to process a batch of B samples, each of shape (N,), you’ll need to stack them into a single tensor of shape (B, N) before passing them to the model. In this case, you won’t need the None index (expansion fcn), because the batch dimension will already be present."
      ],
      "metadata": {
        "id": "HjyVbve9hbvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, game_state, policy, parent=None, move=None):\n",
        "        self.game_state = game_state\n",
        "        self.policy = policy\n",
        "        self.parent = parent\n",
        "        self.move = move\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.wins = 0\n",
        "    def __str__(self):\n",
        "        return f\"GameState: {self.game_state}, Move: {self.move}, Visits: {self.visits}, Wins: {self.wins}\"\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, root):\n",
        "        self.model = model\n",
        "        self.root = root\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=0.1,\n",
        "            decay_steps=100,\n",
        "            decay_rate=0.1\n",
        "        )\n",
        "        self.optimizer.learning_rate = self.scheduler\n",
        "\n",
        "    def choose_action(self, node):\n",
        "        # Check the mode of the model\n",
        "        if self.model.training:\n",
        "            # If the model is in training mode, use probabilistic action selection\n",
        "            visit_counts = np.array([child.visits for child in node.children])\n",
        "            visit_dist = visit_counts / visit_counts.sum()\n",
        "            chosen_action = node.children[np.random.choice(len(node.children), p=visit_dist)].move\n",
        "        else:\n",
        "            # If the model is in evaluation mode, use deterministic action selection\n",
        "            max_visits = max(child.visits for child in node.children)\n",
        "            chosen_action = [child.move for child in node.children if child.visits == max_visits][0]\n",
        "        return chosen_action\n",
        "\n",
        "    def selection(self, node):\n",
        "        # While the current node has children (i.e., it's not a leaf node)\n",
        "        while len(node.children) > 0:\n",
        "            # Initialize the maximum value and the selected node\n",
        "            max_value = -np.inf\n",
        "            selected_node = None\n",
        "\n",
        "            # Loop over each child of the current node\n",
        "            for child in node.children:\n",
        "                # Convert the game state to a tensor\n",
        "                state_tensor = tf.convert_to_tensor(node.game_state.to_array(), dtype=tf.float32)\n",
        "                # Use the neural network to evaluate the game state and estimate the policy and value\n",
        "                policy, value = self.model(state_tensor[None, ...])\n",
        "\n",
        "                # Compute the Q-value (average reward) of the child node\n",
        "                Q = child.wins / child.visits if child.visits != 0 else 0\n",
        "                # Compute the U-value (exploration term) of the child node\n",
        "                # U = tf.reduce_mean(policy) * tf.math.sqrt(node.visits) / (1 + child.visits)\n",
        "                U = tf.reduce_mean(policy) * tf.math.sqrt(tf.cast(node.visits, tf.float32)) / (1 + tf.cast(child.visits, tf.float32))\n",
        "                # The value of the node is the sum of the Q-value and U-value\n",
        "                node_value = Q + U\n",
        "\n",
        "                # If the node's value is greater than the current maximum value\n",
        "                if node_value > max_value:\n",
        "                    # Update the maximum value and the selected node\n",
        "                    max_value = node_value\n",
        "                    selected_node = child\n",
        "\n",
        "            # Move to the selected node\n",
        "            node = selected_node\n",
        "\n",
        "        # Return the final selected node\n",
        "        return node\n",
        "\n",
        "    def expansion(self, node):\n",
        "        # Get the list of legal moves from the game state\n",
        "        legal_moves = node.game_state.get_legal_moves()\n",
        "\n",
        "        # For each legal move, create a new node and add it to the children of the current node\n",
        "        for move in legal_moves:\n",
        "            new_game_state = node.game_state.make_move(move)\n",
        "            state_tensor = tf.convert_to_tensor(new_game_state.to_array(), dtype=tf.float32)\n",
        "            policy, value = self.model(state_tensor[None, ...])\n",
        "            child_node = Node(new_game_state, policy, parent=node, move=move)\n",
        "            node.children.append(child_node)\n",
        "\n",
        "        return node.children\n",
        "\n",
        "    def simulation(self, node):\n",
        "        # Make a copy of the game state\n",
        "        game_state = node.game_state.copy()\n",
        "\n",
        "        # While the game is not over\n",
        "        while not game_state.is_terminal():\n",
        "            # Convert the game state to a tensor\n",
        "            state_tensor = tf.convert_to_tensor(game_state.to_array(), dtype=tf.float32)\n",
        "            # Use the neural network to estimate the policy and value\n",
        "            policy, _ = self.model(state_tensor[None, ...])\n",
        "            # Convert the policy to a probability distribution\n",
        "            policy_dist = tf.nn.softmax(policy).numpy()[0]\n",
        "            # Get the list of legal moves\n",
        "            legal_moves = game_state.get_legal_moves()\n",
        "            # Choose a move based on the policy\n",
        "            move = np.random.choice(legal_moves, p=policy_dist)\n",
        "            # Apply the move to get the next game state\n",
        "            game_state = game_state.make_move(move)\n",
        "\n",
        "        # Return the reward associated with the terminal state\n",
        "        return game_state.get_reward()\n",
        "\n",
        "    def backpropagation(self, node, reward):\n",
        "        # While node is not None\n",
        "        while node is not None:\n",
        "            # Update the visit count of the node\n",
        "            node.visits += 1\n",
        "\n",
        "            # Update the win count of the node\n",
        "            node.wins += reward\n",
        "\n",
        "            # Move to the parent node\n",
        "            node = node.parent\n",
        "\n",
        "    def run(self, simulations):\n",
        "        for _ in range(simulations):\n",
        "            # Start from the root node\n",
        "            node = self.root\n",
        "\n",
        "            # Selection\n",
        "            node = self.selection(node)\n",
        "\n",
        "            # Skip expansion, simulation, and backpropagation if a terminal node is selected\n",
        "            if node.game_state.is_terminal():\n",
        "                continue\n",
        "\n",
        "            # Expansion\n",
        "            if not node.game_state.is_terminal():\n",
        "                node = random.choice(self.expansion(node))\n",
        "\n",
        "            # Simulation\n",
        "            reward = self.simulation(node)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagation(node, reward)\n",
        "\n",
        "            # Choose an action\n",
        "            chosen_action = self.choose_action(node)\n",
        "\n",
        "            # Get the list of children that match the chosen action\n",
        "            matching_children = [child for child in node.children if child.move == chosen_action]\n",
        "\n",
        "            # Check if there are any matching children\n",
        "            if matching_children:\n",
        "                # If there are, set the root to the first matching child\n",
        "                self.root = matching_children[0]\n",
        "            else:\n",
        "                # If there are no matching children, handle the error appropriately\n",
        "                print(\"No child found with the chosen action.\")\n",
        "\n",
        "    def print_tree(self, node, indent=\"\"):\n",
        "        print(indent + str(node))\n",
        "        for child in node.children:\n",
        "            self.print_tree(child, indent + \"  \")\n",
        "\n",
        "    def self_play(self, network, game, game_number, num_simulations=50):\n",
        "        states = []\n",
        "        policies = []\n",
        "        current_state = game.get_initial_state()\n",
        "        root = Node(current_state, None)\n",
        "\n",
        "        while not current_state.is_terminal():\n",
        "            # Perform MCTS simulations from the root\n",
        "            for _ in range(num_simulations):\n",
        "                leaf = self.selection(root)\n",
        "                children = self.expansion(leaf)\n",
        "                reward = self.simulation(random.choice(children))\n",
        "                self.backpropagation(leaf, reward)\n",
        "\n",
        "            # Get the visit counts of the root's children\n",
        "            visit_counts = tf.convert_to_tensor([child.visits for child in root.children], dtype=tf.float32)\n",
        "\n",
        "            # Convert the visit counts to a policy\n",
        "            policy = tf.nn.softmax(visit_counts).numpy().tolist()\n",
        "\n",
        "            # Choose an action based on the policy\n",
        "            action_index = tf.random.categorical(tf.math.log(visit_counts[None, ...]), 1).numpy()[0][0]\n",
        "            action = root.children[action_index].move\n",
        "\n",
        "            # Store the numerical representation of the state and policy\n",
        "            states.append(current_state.to_array())\n",
        "            policies.append(policy)\n",
        "\n",
        "            # Apply the action to get the next state\n",
        "            current_state = current_state.make_move(action)\n",
        "\n",
        "            # Update the root node to the child node corresponding to the chosen action\n",
        "            root = root.children[action_index]\n",
        "\n",
        "        # Get the reward from the final state\n",
        "        reward = current_state.get_reward()\n",
        "\n",
        "        # Perform backpropagation after the game ends\n",
        "        self.backpropagation(root, reward)\n",
        "\n",
        "        return states, policies, reward\n",
        "\n",
        "    def train(self, states, policies, reward, epochs):\n",
        "        # Convert the states, policies, and reward to TensorFlow tensors\n",
        "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "        policies = tf.convert_to_tensor(policies, dtype=tf.float32)\n",
        "        result = tf.convert_to_tensor([reward] * len(states), dtype=tf.float32)[..., None]\n",
        "\n",
        "        # Loop over the number of training epochs\n",
        "        for _ in range(epochs):\n",
        "            # Get the predicted policy and value from the network\n",
        "            with tf.GradientTape() as tape:\n",
        "                policy_pred, value_pred = self.model(states)\n",
        "\n",
        "                # Compute the policy loss as the KL divergence between the predicted and target policy\n",
        "                policy_loss = tf.keras.losses.KLDivergence()(policies, tf.nn.softmax(policy_pred))\n",
        "                # Compute the value loss as the mean squared error between the predicted and actual reward\n",
        "                value_loss = tf.keras.losses.MSE(result, value_pred)\n",
        "                # The total loss is the sum of the policy loss and value loss\n",
        "                loss = policy_loss + value_loss\n",
        "\n",
        "            # Compute gradients and update the network parameters\n",
        "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "        # Return the final loss\n",
        "        return loss.numpy()"
      ],
      "metadata": {
        "id": "lze5NhxUdN8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_channels):\n",
        "        super().__init__()\n",
        "        self.fc1 = tf.keras.layers.Dense(num_channels, activation='relu')\n",
        "        self.fc2 = tf.keras.layers.Dense(num_channels)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.fc1(inputs)\n",
        "        x = self.fc2(x)\n",
        "        x += inputs  # Skip connection\n",
        "        return tf.nn.relu(x)\n",
        "\n",
        "class PolicyValueResNet(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "        self.hidden1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.resblock = ResidualBlock(64)\n",
        "        self.hidden2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "\n",
        "        # Policy Head forms probabilities for each action, distribution\n",
        "        self.policy_head = tf.keras.layers.Dense(num_actions)\n",
        "\n",
        "        # Value Head predicts winner of game from each position, scaler\n",
        "        self.value_head = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.hidden1(state)\n",
        "        x = self.resblock(x)\n",
        "        x = self.hidden2(x)\n",
        "        policy = tf.nn.softmax(self.policy_head(x))\n",
        "        value = tf.math.tanh(self.value_head(x))\n",
        "        return policy, value"
      ],
      "metadata": {
        "id": "7BWTkgx2dPSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Training Loop"
      ],
      "metadata": {
        "id": "hjlaQcU0dmXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the game and network\n",
        "game = GameState(0)\n",
        "num_actions = len(game.get_legal_moves())\n",
        "network = PolicyValueResNet(num_actions)\n",
        "\n",
        "# Create a uniform policy\n",
        "uniform_policy = [1.0 / num_actions] * num_actions\n",
        "\n",
        "root = Node(game.get_initial_state(), uniform_policy)\n",
        "\n",
        "# Create the MCTS\n",
        "mcts = MCTS(network, root)\n",
        "\n",
        "# Initialize a list to store the losses\n",
        "losses = []\n",
        "\n",
        "epochs = 1 # Play games\n",
        "game_number = 1\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.1,\n",
        "    decay_steps=100,\n",
        "    decay_rate=0.1\n",
        ")\n",
        "optimizer.learning_rate = scheduler\n",
        "\n",
        "# Generate self-play data and train the network\n",
        "for i in range(epochs):\n",
        "    states, actions, reward = mcts.self_play(network, game, game_number)\n",
        "    loss = mcts.train(states, actions, reward, epochs)\n",
        "    losses.append(loss)\n",
        "    game_number += 1\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler(optimizer.iterations)"
      ],
      "metadata": {
        "id": "TE6bSqLFdZdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save - AddingGame_MCTS_Model2.pth"
      ],
      "metadata": {
        "id": "LAF2Ys-ZCFdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the checkpoint and the checkpoint manager\n",
        "checkpoint = tf.train.Checkpoint(model=network, optimizer=mcts.optimizer, scheduler=scheduler)\n",
        "checkpoint_manager = tf.train.CheckpointManager(checkpoint, '/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.ckpt', max_to_keep=5)\n",
        "\n",
        "# Save the model, optimizer, game number, and losses\n",
        "checkpoint.game_number = game_number\n",
        "checkpoint.losses = losses\n",
        "checkpoint_manager.save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "o5h2irDtCI1h",
        "outputId": "7cfcf353-8d0b-4a31-f7b4-16fe8e43daba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`Checkpoint` was expecting scheduler to be a trackable object (an object derived from `Trackable`), got <keras.optimizers.schedules.learning_rate_schedule.ExponentialDecay object at 0x7d9c50406b00>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5887b0723b6d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the checkpoint and the checkpoint manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcheckpoint_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpointManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, **kwargs)\u001b[0m\n\u001b[1;32m   2159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \u001b[0mconverted_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m       \u001b[0m_assert_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/checkpoint/checkpoint.py\u001b[0m in \u001b[0;36m_assert_trackable\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m   1519\u001b[0m   if not isinstance(\n\u001b[1;32m   1520\u001b[0m       obj, (base.Trackable, def_function.Function)):\n\u001b[0;32m-> 1521\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1522\u001b[0m         \u001b[0;34mf\"`Checkpoint` was expecting {name} to be a trackable object (an \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0;34mf\"object derived from `Trackable`), got {obj}. If you believe this \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `Checkpoint` was expecting scheduler to be a trackable object (an object derived from `Trackable`), got <keras.optimizers.schedules.learning_rate_schedule.ExponentialDecay object at 0x7d9c50406b00>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load & Continue - AddingGame_MCTS_Model2.pth"
      ],
      "metadata": {
        "id": "yXPZBNPXCU7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create the game and network\n",
        "game = GameState(0)\n",
        "num_actions = len(game.get_legal_moves())\n",
        "network = PolicyValueResNet(num_actions)\n",
        "\n",
        "# Create a uniform policy\n",
        "uniform_policy = [1.0 / num_actions] * num_actions\n",
        "\n",
        "root = Node(game.get_initial_state(), uniform_policy)\n",
        "\n",
        "# Create the MCTS\n",
        "mcts = MCTS(network, root)\n",
        "\n",
        "# Define the checkpoint and the checkpoint manager\n",
        "checkpoint = tf.train.Checkpoint(model=network, optimizer=mcts.optimizer, scheduler=mcts.scheduler)\n",
        "checkpoint_manager = tf.train.CheckpointManager(checkpoint, '/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.ckpt', max_to_keep=5)\n",
        "\n",
        "# Restore the latest checkpoint\n",
        "checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
        "\n",
        "game_number = checkpoint.game_number.numpy()\n",
        "losses = checkpoint.losses.numpy().tolist()\n",
        "\n",
        "# Continue training for...<epochs> more games\n",
        "epochs=800\n",
        "\n",
        "for _ in range(epochs):\n",
        "    states, actions, reward = mcts.self_play(network, game, game_number)\n",
        "    loss = mcts.train(states, actions, reward, epochs)\n",
        "    losses.append(loss)\n",
        "    game_number += 1\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    mcts.scheduler(mcts.optimizer.iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwmUGLMZCZqW",
        "outputId": "014256c6-826d-4ef3-fdf1-2dfcbbadc8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot"
      ],
      "metadata": {
        "id": "wgQNH257CsqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Print and plot the losses\n",
        "print(f\"Average Loss: {sum(losses) / len(losses)}, Max Loss: {max(losses)}, Min Loss: {min(losses)}\")\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Game')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss per Game')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O2Waf7sMeDci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play, Eval() - AddingGame_MCTS_Model2.pth"
      ],
      "metadata": {
        "id": "mCTgDDkAeI_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the results of each game\n",
        "win_rates = []\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# See the model playout 10 games using MCTS\n",
        "\n",
        "# Create the game and network\n",
        "game = GameState(0)\n",
        "num_actions = len(game.get_legal_moves())\n",
        "network = PolicyValueResNet(num_actions)\n",
        "\n",
        "# Create a uniform policy\n",
        "uniform_policy = [1.0 / num_actions] * num_actions\n",
        "\n",
        "root = Node(game.get_initial_state(), uniform_policy)\n",
        "\n",
        "# Create the MCTS\n",
        "mcts = MCTS(network, root)\n",
        "\n",
        "# Define the checkpoint and the checkpoint manager\n",
        "checkpoint = tf.train.Checkpoint(model=network, optimizer=mcts.optimizer, scheduler=mcts.scheduler)\n",
        "checkpoint_manager = tf.train.CheckpointManager(checkpoint, '/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.ckpt', max_to_keep=5)\n",
        "\n",
        "# Restore the latest checkpoint\n",
        "checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
        "\n",
        "game_number = checkpoint.game_number.numpy()\n",
        "losses = checkpoint.losses.numpy().tolist()\n",
        "\n",
        "# Simulate 10 games\n",
        "num_games = 10\n",
        "\n",
        "for i in range(num_games):\n",
        "    print(f\"Game {i + 1}:\")\n",
        "\n",
        "    # Initialize the game and the root node\n",
        "    game_state = game.get_initial_state()\n",
        "    root = Node(game_state, None)\n",
        "\n",
        "    # Make sure that the MCTS and the root node persist throughout each game\n",
        "    mcts = MCTS(network, root)\n",
        "\n",
        "    while not game_state.is_terminal():\n",
        "        # Perform MCTS simulations from the root\n",
        "        for _ in range(num_games):\n",
        "            leaf = mcts.selection(root)\n",
        "            children = mcts.expansion(leaf)\n",
        "            reward = mcts.simulation(random.choice(children))\n",
        "            mcts.backpropagation(leaf, reward)\n",
        "\n",
        "        # Choose the action that leads to the most visited child node\n",
        "        action = mcts.choose_action(root)\n",
        "\n",
        "        # Apply the action to get the next state\n",
        "        game_state = game_state.make_move(action)\n",
        "\n",
        "        print(f\"Action taken: {action}\")\n",
        "\n",
        "    print(f\"Final reward: {reward}\")\n",
        "\n",
        "    # Append the result of the game to the win_rates list\n",
        "    # Assume a reward of 1 is a win, 0 is a draw, and -1 is a loss\n",
        "    win_rates.append(reward)\n",
        "\n",
        "# Calculate the win rate\n",
        "win_rate = win_rates.count(1) / num_games\n",
        "\n",
        "# Print and plot the win rates\n",
        "print(f\"Win Rate: {win_rate}\")\n",
        "plt.plot(range(1, num_games + 1), win_rates)\n",
        "plt.xlabel('Game')\n",
        "plt.ylabel('Win Rate')\n",
        "plt.title('Win Rate per Game')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EgTki4vsePN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}