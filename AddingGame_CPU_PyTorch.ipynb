{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSVph1HoFO/o6T5UvGxzTn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emm32449/MCTS-in-Python/blob/main/AddingGame_CPU_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The game is a simple adding game. The game starts at 0 and players can make moves by adding 1, 2, or 3 to the current state. The goal is to reach exactly 10. If a player’s move results in a state exceeding 10, they receive a negative reward. If the state is less than 10, the reward is based on how close the state is to 10. The game resets once it reaches or exceeds 10."
      ],
      "metadata": {
        "id": "WAZmLtNwwWoM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0QYmzhw8vvl"
      },
      "outputs": [],
      "source": [
        "# Assume we have a simple game state\n",
        "class GameState:\n",
        "    def __init__(self, state):\n",
        "        self.state = state\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.state)\n",
        "\n",
        "    def get_legal_moves(self):\n",
        "        return [1, 2, 3]\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        return GameState(0)\n",
        "\n",
        "    def make_move(self, move):\n",
        "        return GameState(self.state + move)\n",
        "\n",
        "    def is_terminal(self):\n",
        "        return self.state >= 10\n",
        "\n",
        "    def get_reward(self):\n",
        "        # Give a positive reward if the state is exactly 10\n",
        "        if self.state == 10:\n",
        "            return 1\n",
        "        # Give a negative reward if the state exceeds 10\n",
        "        elif self.state > 10:\n",
        "            return -1\n",
        "        # Give a reward based on how close the state is to 10\n",
        "        else:\n",
        "            return 1 - abs(self.state - 10) / 10\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "\n",
        "    def copy(self):\n",
        "        return GameState(self.state)\n",
        "\n",
        "    def to_array(self):\n",
        "        return [self.state]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alpha Go Zero uses a variant of Monte Carlo Tree Search (MCTS) within its training loop. Starting from the root node, which represents the current game state, it traverses the tree until it reaches a leaf node. This game state is passed through the neural network, which outputs a vector of move probabilities and a scalar estimating the expected outcome. The leaf node is then expanded, with each legal move becoming a new child node. The move probabilities output by the network initialize the prior probabilities of selecting each child node in future simulations. The estimated value output by the network is backpropagated up the tree, updating the value estimates of all nodes along the traversed path.\n",
        "\n",
        "This process is repeated for many simulations, with the Upper Confidence Bound (UCB) formula guiding the selection of nodes during tree traversal. The UCB formula balances exploration and exploitation, taking into account both the value of the node and the prior probability of the node, as well as the number of times the node has been visited. After a large number of simulations, the move leading to the child node with the highest visit count from the root is selected as the next action. This procedure allows AlphaGo Zero to effectively learn a policy favoring high-value actions and a value function predicting the game outcome."
      ],
      "metadata": {
        "id": "lFNyawMAyaHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, game_state, policy, parent=None, move=None):\n",
        "        self.game_state = game_state\n",
        "        self.policy = policy\n",
        "        self.parent = parent\n",
        "        self.move = move\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.wins = 0\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"GameState: {self.game_state}, Move: {self.move}, Visits: {self.visits}, Wins: {self.wins}\"\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, root):\n",
        "        self.model = model\n",
        "        self.root = root\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
        "\n",
        "    def choose_action(self, node):\n",
        "        # Check the mode of the model\n",
        "        if self.model.training:\n",
        "            # If the model is in training mode, use probabilistic action selection\n",
        "            visit_counts = np.array([child.visits for child in node.children])\n",
        "            visit_dist = visit_counts / visit_counts.sum()\n",
        "            chosen_action = node.children[np.random.choice(len(node.children), p=visit_dist)].move\n",
        "        else:\n",
        "            # If the model is in evaluation mode, use deterministic action selection\n",
        "            max_visits = max(child.visits for child in node.children)\n",
        "            chosen_action = [child.move for child in node.children if child.visits == max_visits][0]\n",
        "        return chosen_action\n",
        "\n",
        "    def selection(self, node):\n",
        "        # While the current node has children (i.e., it's not a leaf node)\n",
        "        while len(node.children) > 0:\n",
        "            # Initialize the maximum value and the selected node\n",
        "            max_value = -math.inf\n",
        "            selected_node = None\n",
        "\n",
        "            # Loop over each child of the current node\n",
        "            for child in node.children:\n",
        "                # Convert the game state to a tensor\n",
        "                state_tensor = torch.tensor(node.game_state.to_array(), dtype=torch.float32)\n",
        "                # Use the neural network to evaluate the game state and estimate the policy and value\n",
        "                policy, value = self.model(state_tensor)\n",
        "\n",
        "                # Compute the Q-value (average reward) of the child node\n",
        "                Q = child.wins / child.visits if child.visits != 0 else 0\n",
        "                # Compute the U-value (exploration term) of the child node\n",
        "                U = policy.mean() * math.sqrt(node.visits) / (1 + child.visits)\n",
        "                # The value of the node is the sum of the Q-value and U-value\n",
        "                node_value = Q + U\n",
        "\n",
        "                # If the node's value is greater than the current maximum value\n",
        "                if node_value > max_value:\n",
        "                    # Update the maximum value and the selected node\n",
        "                    max_value = node_value\n",
        "                    selected_node = child\n",
        "\n",
        "            # Move to the selected node\n",
        "            node = selected_node\n",
        "\n",
        "        # Return the final selected node\n",
        "        return node\n",
        "\n",
        "    def expansion(self, node):\n",
        "        # Get the list of legal moves from the game state\n",
        "        legal_moves = node.game_state.get_legal_moves()\n",
        "\n",
        "        # For each legal move, create a new node and add it to the children of the current node\n",
        "        for move in legal_moves:\n",
        "            new_game_state = node.game_state.make_move(move)\n",
        "            state_tensor = torch.tensor(new_game_state.to_array(), dtype=torch.float32)\n",
        "            policy, value = self.model(state_tensor)\n",
        "            child_node = Node(new_game_state, policy, parent=node, move=move)\n",
        "            node.children.append(child_node)\n",
        "\n",
        "        return node.children\n",
        "\n",
        "    def simulation(self, node):\n",
        "        # Make a copy of the game state\n",
        "        game_state = node.game_state.copy()\n",
        "\n",
        "        # While the game is not over\n",
        "        while not game_state.is_terminal():\n",
        "            # Convert the game state to a tensor\n",
        "            state_tensor = torch.tensor(game_state.to_array(), dtype=torch.float32)\n",
        "            # Use the neural network to estimate the policy and value\n",
        "            policy, _ = self.model(state_tensor)\n",
        "            # Convert the policy to a probability distribution\n",
        "            policy_dist = F.softmax(policy, dim=0).detach().numpy()\n",
        "            # Get the list of legal moves\n",
        "            legal_moves = game_state.get_legal_moves()\n",
        "            # Choose a move based on the policy\n",
        "            move = np.random.choice(legal_moves, p=policy_dist)\n",
        "            # Apply the move to get the next game state\n",
        "            game_state = game_state.make_move(move)\n",
        "\n",
        "        # Return the reward associated with the terminal state\n",
        "        return game_state.get_reward()\n",
        "\n",
        "    def backpropagation(self, node, reward):\n",
        "        # While node is not None\n",
        "        while node is not None:\n",
        "            # Update the visit count of the node\n",
        "            node.visits += 1\n",
        "\n",
        "            # Update the win count of the node\n",
        "            node.wins += reward\n",
        "\n",
        "            # Move to the parent node\n",
        "            node = node.parent\n",
        "\n",
        "    def run(self, simulations):\n",
        "        for _ in range(simulations):\n",
        "            # Start from the root node\n",
        "            node = self.root\n",
        "\n",
        "            # Selection\n",
        "            node = self.selection(node)\n",
        "\n",
        "            # Skip expansion, simulation, and backpropagation if a terminal node is selected\n",
        "            if node.game_state.is_terminal():\n",
        "                continue\n",
        "\n",
        "            # Expansion\n",
        "            if not node.game_state.is_terminal():\n",
        "                node = random.choice(self.expansion(node))\n",
        "\n",
        "            # Simulation\n",
        "            reward = self.simulation(node)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backpropagation(node, reward)\n",
        "\n",
        "            # Choose an action\n",
        "            chosen_action = self.choose_action(node)\n",
        "\n",
        "            # Get the list of children that match the chosen action\n",
        "            matching_children = [child for child in node.children if child.move == chosen_action]\n",
        "\n",
        "            # Check if there are any matching children\n",
        "            if matching_children:\n",
        "                # If there are, set the root to the first matching child\n",
        "                self.root = matching_children[0]\n",
        "            else:\n",
        "                # If there are no matching children, handle the error appropriately\n",
        "                print(\"No child found with the chosen action.\")\n",
        "\n",
        "    def print_tree(self, node, indent=\"\"):\n",
        "        print(indent + str(node))\n",
        "        for child in node.children:\n",
        "            self.print_tree(child, indent + \"  \")\n",
        "\n",
        "    def self_play(self, network, game, game_number, num_simulations=50):\n",
        "        states = []\n",
        "        policies = []\n",
        "        current_state = game.get_initial_state()\n",
        "        root = Node(current_state, None)\n",
        "\n",
        "        while not current_state.is_terminal():\n",
        "            # Perform MCTS simulations from the root\n",
        "            for _ in range(num_simulations):\n",
        "                leaf = self.selection(root)\n",
        "                children = self.expansion(leaf)\n",
        "                reward = self.simulation(random.choice(children))\n",
        "                self.backpropagation(leaf, reward)\n",
        "\n",
        "            # Get the visit counts of the root's children\n",
        "            visit_counts = torch.tensor([child.visits for child in root.children]).float()\n",
        "\n",
        "            # Convert the visit counts to a policy\n",
        "            policy = F.softmax(visit_counts, dim=0).tolist()\n",
        "\n",
        "            # Choose an action based on the policy\n",
        "            action_index = torch.multinomial(visit_counts, 1).item()\n",
        "            action = root.children[action_index].move\n",
        "\n",
        "            # Store the numerical representation of the state and policy\n",
        "            states.append(current_state.to_array())\n",
        "            policies.append(policy)\n",
        "\n",
        "            # Apply the action to get the next state\n",
        "            current_state = current_state.make_move(action)\n",
        "\n",
        "            # Update the root node to the child node corresponding to the chosen action\n",
        "            root = root.children[action_index]\n",
        "\n",
        "        # Get the reward from the final state\n",
        "        reward = current_state.get_reward()\n",
        "\n",
        "        # Perform backpropagation after the game ends\n",
        "        self.backpropagation(root, reward)\n",
        "\n",
        "        return states, policies, reward\n",
        "\n",
        "    # Use the improved move probabilities from MCTS as targets to train the neural network\n",
        "    # This function trains the neural network using the states, policies, and reward from self-play\n",
        "    def train(self, states, policies, reward, epochs):\n",
        "        # Convert the states, policies, and reward to PyTorch tensors\n",
        "        states = torch.tensor(states, dtype=torch.float32)\n",
        "        policies = torch.tensor(policies, dtype=torch.float32)\n",
        "        result = torch.tensor([reward] * len(states), dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "        # Loop over the number of training epochs\n",
        "        for _ in range(epochs):\n",
        "            # Get the predicted policy and value from the network\n",
        "            policy_pred, value_pred = self.model(states)\n",
        "\n",
        "            # Compute the policy loss as the KL divergence between the predicted and target policy\n",
        "            policy_loss = F.kl_div(F.log_softmax(policy_pred, dim=1), policies)\n",
        "            # Compute the value loss as the mean squared error between the predicted and actual reward\n",
        "            value_loss = F.mse_loss(value_pred, result)\n",
        "            # The total loss is the sum of the policy loss and value loss\n",
        "            loss = policy_loss + value_loss\n",
        "\n",
        "            # Zero the gradients, perform backpropagation, and update the network parameters\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Return the final loss\n",
        "        return loss.item()"
      ],
      "metadata": {
        "id": "sOOxRknh73dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input is a 1D vector rather than a 2D Board or image. Adjust the network to use fully connected layers (nn.Linear) instead of 2D convolutional layers (nn.Conv2d). The idea behind a Residual Block is to introduce a so-called “skip connection” or “shortcut”, which allows the gradient to be directly backpropagated to earlier layers."
      ],
      "metadata": {
        "id": "XRFKPBfHv8ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, num_channels):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(num_channels, num_channels)\n",
        "        self.fc2 = nn.Linear(num_channels, num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(out)\n",
        "        out += x  # Skip connection\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class PolicyValueResNet(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "        self.hidden1 = nn.Linear(1, 64)\n",
        "        self.resblock = ResidualBlock(64)\n",
        "        self.hidden2 = nn.Linear(64, 64)\n",
        "\n",
        "        # Policy Head forms probabilities for each action, distribution\n",
        "        self.policy_head = nn.Linear(64, num_actions)\n",
        "\n",
        "        # Value Head predicts winner of game from each position, scaler\n",
        "        self.value_head = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.hidden1(state))\n",
        "        x = self.resblock(x)\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        policy = F.softmax(self.policy_head(x), dim=-1)\n",
        "        value = torch.tanh(self.value_head(x))\n",
        "        return policy, value"
      ],
      "metadata": {
        "id": "pru2HloYByJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Training Loop"
      ],
      "metadata": {
        "id": "2DtoYH_eB9lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the game and network\n",
        "game = GameState(0)\n",
        "num_actions = len(game.get_legal_moves())\n",
        "network = PolicyValueNet(num_actions)\n",
        "\n",
        "# Create a uniform policy\n",
        "uniform_policy = [1.0 / num_actions] * num_actions\n",
        "\n",
        "root = Node(game.get_initial_state(), uniform_policy)\n",
        "\n",
        "# Create the MCTS\n",
        "mcts = MCTS(network, root)\n",
        "\n",
        "# Initialize a list to store the losses\n",
        "losses = []\n",
        "\n",
        "# Set the model to training mode\n",
        "network.train()\n",
        "\n",
        "epochs = 100 # Play games\n",
        "game_number = 1\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(network.parameters())\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
        "\n",
        "# Generate self-play data and train the network\n",
        "for i in range(epochs):\n",
        "    states, actions, reward = mcts.self_play(network, game, game_number)\n",
        "    loss = mcts.train(states, actions, reward, epochs)\n",
        "    losses.append(loss)\n",
        "    game_number += 1\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc9tvyZeB6zh",
        "outputId": "c3345612-921d-4ca6-b126-21b96cbe55a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save - AddingGame_MCTS_Model2.pth"
      ],
      "metadata": {
        "id": "LAF2Ys-ZCFdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save the model, optimizer, game number, and losses\n",
        "torch.save({\n",
        "    'model_state_dict': network.state_dict(),\n",
        "    'optimizer_state_dict': mcts.optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'game_number': game_number,\n",
        "    'losses': losses\n",
        "}, '/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5h2irDtCI1h",
        "outputId": "3b83fbf7-92d6-4d1e-c7ad-88e262dd704d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load & Continue - AddingGame_MCTS_Model2.pth"
      ],
      "metadata": {
        "id": "yXPZBNPXCU7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create the game and network\n",
        "game = GameState(0)\n",
        "num_actions = len(game.get_legal_moves())\n",
        "network = PolicyValueNet(num_actions)\n",
        "\n",
        "# Load the model, optimizer, game number, and losses\n",
        "checkpoint = torch.load('/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.pth')\n",
        "network.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Create a uniform policy\n",
        "uniform_policy = [1.0 / num_actions] * num_actions\n",
        "\n",
        "root = Node(game.get_initial_state(), uniform_policy)\n",
        "\n",
        "# Create the MCTS\n",
        "mcts = MCTS(network, root)\n",
        "mcts.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "mcts.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "game_number = checkpoint['game_number']\n",
        "losses = checkpoint['losses']\n",
        "\n",
        "# Set the model to training mode\n",
        "network.train()\n",
        "\n",
        "# Continue training for...<epochs> more games\n",
        "epochs=800\n",
        "\n",
        "for _ in range(epochs):\n",
        "    states, actions, reward = mcts.self_play(network, game, game_number)\n",
        "    loss = mcts.train(states, actions, reward, epochs)\n",
        "    losses.append(loss)\n",
        "    game_number += 1\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwmUGLMZCZqW",
        "outputId": "014256c6-826d-4ef3-fdf1-2dfcbbadc8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot"
      ],
      "metadata": {
        "id": "wgQNH257CsqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Print and plot the losses\n",
        "print(f\"Average Loss: {sum(losses) / len(losses)}, Max Loss: {max(losses)}, Min Loss: {min(losses)}\")\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Game')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss per Game')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UPoohsmUCrX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play, Eval() - AddingGame_MCTS_Model2.pth"
      ],
      "metadata": {
        "id": "VjoxJ-3iCvEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store the results of each game\n",
        "win_rates = []\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# See the model playout 10 games using MCTS\n",
        "\n",
        "# Load the model and optimizer\n",
        "checkpoint = torch.load('/content/drive/My Drive/MCTS in Python/AddingGame_MCTS_Model2.pth')\n",
        "network.load_state_dict(checkpoint['model_state_dict'])\n",
        "mcts.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "network.eval()\n",
        "\n",
        "# Simulate 10 games\n",
        "num_games = 10\n",
        "\n",
        "for i in range(num_games):\n",
        "    print(f\"Game {i + 1}:\")\n",
        "\n",
        "    # Initialize the game and the root node\n",
        "    game_state = game.get_initial_state()\n",
        "    root = Node(game_state, None)\n",
        "\n",
        "    #Make sure that the MCTS and the root node persist throughout each game\n",
        "    mcts = MCTS(network, root)\n",
        "\n",
        "    while not game_state.is_terminal():\n",
        "        # Perform MCTS simulations from the root\n",
        "        for _ in range(num_games):\n",
        "            leaf = mcts.selection(root)\n",
        "            children = mcts.expansion(leaf)\n",
        "            reward = mcts.simulation(random.choice(children))\n",
        "            mcts.backpropagation(leaf, reward)\n",
        "\n",
        "        # Choose the action that leads to the most visited child node\n",
        "        action = mcts.choose_action(root)\n",
        "\n",
        "        # Apply the action to get the next state\n",
        "        game_state = game_state.make_move(action)\n",
        "\n",
        "        print(f\"Action taken: {action}\")\n",
        "\n",
        "    print(f\"Final reward: {reward}\")\n",
        "\n",
        "    # Append the result of the game to the win_rates list\n",
        "    # Assume a reward of 1 is a win, 0 is a draw, and -1 is a loss\n",
        "    win_rates.append(reward)\n",
        "\n",
        "# mcts.print_tree(mcts.root)\n",
        "\n",
        "# Calculate the win rate\n",
        "win_rate = win_rates.count(1) / num_games\n",
        "\n",
        "# Print and plot the win rates\n",
        "print(f\"Win Rate: {win_rate}\")\n",
        "plt.plot(range(1, num_games + 1), win_rates)\n",
        "plt.xlabel('Game')\n",
        "plt.ylabel('Win Rate')\n",
        "plt.title('Win Rate per Game')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MwqfjyLTCyTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test shows an example training with PyTorch and **without** CPU Multiprocessing:"
      ],
      "metadata": {
        "id": "_DNx6MOnsnIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_data = datasets.MNIST(root='MNIST/', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='MNIST/', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Define the model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.hidden = nn.Linear(28*28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.hidden(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(5):\n",
        "    for images, labels in DataLoader(train_data, batch_size=64):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the total time taken\n",
        "total_time = end_time - start_time\n",
        "print(f'Total time for training: {total_time} seconds')\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in DataLoader(test_data, batch_size=64):\n",
        "        output = model(images)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('\\nTest accuracy:', correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7767e184-cc46-406a-a6b0-c9dead4094a7",
        "id": "mrLMo-bUsnI5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time for training: 54.97133207321167 seconds\n",
            "\n",
            "Test accuracy: 0.9641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test shows an example training with PyTorch and CPU Multiprocessing:"
      ],
      "metadata": {
        "id": "lnAZ1RLmrobB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_data = datasets.MNIST(root='MNIST/', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = datasets.MNIST(root='MNIST/', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Define the model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.hidden = nn.Linear(28*28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.output = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.hidden(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(5):\n",
        "    for images, labels in DataLoader(train_data, batch_size=64, num_workers=2):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the total time taken\n",
        "total_time = end_time - start_time\n",
        "print(f'Total time for training: {total_time} seconds')\n",
        "\n",
        "# Evaluate the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in DataLoader(test_data, batch_size=64):\n",
        "        output = model(images)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('\\nTest accuracy:', correct / total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DFg9iiTrvih",
        "outputId": "cc490b40-c706-437d-f830-cfcf38ff33fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 47663809.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 7300409.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 46131776.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1170899.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time for training: 69.77310514450073 seconds\n",
            "\n",
            "Test accuracy: 0.9701\n"
          ]
        }
      ]
    }
  ]
}